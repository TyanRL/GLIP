{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04db44ad-0853-45ff-9885-f802df652b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "e:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\layers\\deform_conv.py:313: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "e:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\layers\\deform_conv.py:378: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "e:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\layers\\deform_conv.py:428: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "e:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\layers\\set_loss.py:144: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "e:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\layers\\set_loss.py:326: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "e:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "e:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\modeling\\rpn\\loss.py:94: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "e:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\modeling\\rpn\\loss.py:229: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "e:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\modeling\\rpn\\loss.py:450: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "e:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\modeling\\rpn\\loss.py:847: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "e:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\modeling\\roi_heads\\box_head\\inference.py:37: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "e:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\modeling\\roi_heads\\box_head\\loss.py:122: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "e:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\modeling\\roi_heads\\box_head\\box_head.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TyanRL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\TyanRL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "## imports\n",
    "import glob, os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "pylab.rcParams['figure.figsize'] = 20, 12\n",
    "from maskrcnn_benchmark.config import cfg\n",
    "from maskrcnn_benchmark.engine.predictor_glip import GLIPDemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7b136a7-7144-493c-9545-473ae6a60154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file):\n",
    "    pil_image = Image.open(file).convert(\"RGB\")\n",
    "    # convert to BGR format\n",
    "    image = np.array(pil_image)[:, :, [2, 1, 0]]\n",
    "    return image\n",
    "\n",
    "def load_url(url):\n",
    "    \"\"\"\n",
    "    Given an url of an image, downloads the image and\n",
    "    returns a PIL image\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    pil_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    # convert to BGR format\n",
    "    image = np.array(pil_image)[:, :, [2, 1, 0]]\n",
    "    return image\n",
    "\n",
    "def imshow(img, caption):\n",
    "    plt.imshow(img[:, :, [2, 1, 0]])\n",
    "    plt.axis(\"off\")\n",
    "    plt.figtext(0.5, 0.09, caption, wrap=True, horizontalalignment='center', fontsize=20)\n",
    "\n",
    "def imsave(img, path):\n",
    "    plt.imsave(path, img[:, :, [2, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ab25279-8e3e-463f-96c2-f82e07833565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VISION BACKBONE USE GRADIENT CHECKPOINTING:  False\n",
      "LANGUAGE BACKBONE USE GRADIENT CHECKPOINTING:  False\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\utils\\checkpoint.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=torch.device(\"cuda\"))\n"
     ]
    }
   ],
   "source": [
    "config_file = \"configs/pretrain/glip_Swin_L.yaml\"\n",
    "weight_file = \"../models/glip_large_model.pth\"\n",
    "\n",
    "# update the config options with the config file\n",
    "# manual override some options\n",
    "cfg.local_rank = 0\n",
    "cfg.num_gpus = 1\n",
    "cfg.merge_from_file(config_file)\n",
    "cfg.merge_from_list([\"MODEL.WEIGHT\", weight_file])\n",
    "cfg.merge_from_list([\"MODEL.DEVICE\", \"cuda\"])\n",
    "\n",
    "glip_demo = GLIPDemo(\n",
    "    cfg,\n",
    "    min_image_size=800,\n",
    "    confidence_threshold=0.7,\n",
    "    show_mask_heatmaps=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abfcd1a7-3e95-4fc5-ae96-7a240450f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#caption = 'person . bicycle . car . motorcycle . airplane . bus . train . truck . boat . traffic light . fire hydrant . stop sign . parking meter . bench . bird . cat . dog . horse . sheep . cow . elephant . bear . zebra . giraffe . backpack . umbrella . handbag . tie . suitcase . frisbee . skis . snowboard . sports ball . kite . baseball bat . baseball glove . skateboard . surfboard . tennis racket . bottle . wine glass . cup . fork . knife . spoon . bowl . banana . apple . sandwich . orange . broccoli . carrot . hot dog . pizza . donut . cake . chair . couch . potted plant . bed . dining table . toilet . tv . laptop . mouse . remote . keyboard . cell phone . microwave . oven . toaster . sink . refrigerator . book . clock . vase . scissors . teddy bear . hair drier . toothbrush .'\n",
    "#caption = ['man in back wearing grey shirt . women walking in background . ', 'man in yellow shirt . blue shirt . ']\n",
    "caption = ['small ID card with photo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ec73d2a-fa6e-48d6-9fe3-f3aa6a89e63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 13]], [[19, 24]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "e:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\transformers\\modeling_utils.py:1161: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Not compiled with GPU support\nException raised from modulated_deform_conv_forward at E:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\csrc\\deform_conv.h:145 (most recent call first):\n00007FFEAA07462900007FFEAA074580 c10.dll!c10::Error::Error [<unknown file> @ <unknown line number>]\n00007FFEAA07416A00007FFEAA074110 c10.dll!c10::detail::torchCheckFail [<unknown file> @ <unknown line number>]\n00007FFDB3D7ABFA00007FFDB3D65C60 _C.cp312-win_amd64.pyd!c10::ivalue::Object::operator= [<unknown file> @ <unknown line number>]\n00007FFDB3D85C5100007FFDB3D7AE20 _C.cp312-win_amd64.pyd!PyInit__C [<unknown file> @ <unknown line number>]\n00007FFDB3D8238000007FFDB3D7AE20 _C.cp312-win_amd64.pyd!PyInit__C [<unknown file> @ <unknown line number>]\n00007FFDB3D8241400007FFDB3D7AE20 _C.cp312-win_amd64.pyd!PyInit__C [<unknown file> @ <unknown line number>]\n00007FFDB3D77E8D00007FFDB3D65C60 _C.cp312-win_amd64.pyd!c10::ivalue::Object::operator= [<unknown file> @ <unknown line number>]\n00007FFEACDC28E300007FFEACDC1A80 python312.dll!PyCFunction_GetFlags [<unknown file> @ <unknown line number>]\n00007FFEACD74DFE00007FFEACD74CC0 python312.dll!PyObject_MakeTpCall [<unknown file> @ <unknown line number>]\n00007FFEACD7512500007FFEACD750F0 python312.dll!PyObject_Vectorcall [<unknown file> @ <unknown line number>]\n00007FFEACE9985400007FFEACE92FF0 python312.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFEACD753F400007FFEACD753A0 python312.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n00007FFEACD7507600007FFEACD74EE0 python312.dll!PyVectorcall_Function [<unknown file> @ <unknown line number>]\n00007FFEACD751B600007FFEACD75170 python312.dll!PyObject_Call [<unknown file> @ <unknown line number>]\n00007FFDBAA368B100007FFDBAA23270 torch_python.dll!THPPointer<_frame>::THPPointer<_frame> [<unknown file> @ <unknown line number>]\n00007FFEACDC291E00007FFEACDC1A80 python312.dll!PyCFunction_GetFlags [<unknown file> @ <unknown line number>]\n00007FFEACD7524600007FFEACD75170 python312.dll!PyObject_Call [<unknown file> @ <unknown line number>]\n00007FFEACE99BC300007FFEACE92FF0 python312.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFEACD753F400007FFEACD753A0 python312.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n00007FFEACD77AB100007FFEACD776A0 python312.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n00007FFEACD7810200007FFEACD78020 python312.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n00007FFEACD74F9900007FFEACD74EE0 python312.dll!PyVectorcall_Function [<unknown file> @ <unknown line number>]\n00007FFEACD751B600007FFEACD75170 python312.dll!PyObject_Call [<unknown file> @ <unknown line number>]\n00007FFEACE99BC300007FFEACE92FF0 python312.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFEACD753F400007FFEACD753A0 python312.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n00007FFEACD77AB100007FFEACD776A0 python312.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n00007FFEACD7810200007FFEACD78020 python312.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n00007FFEACD74F9900007FFEACD74EE0 python312.dll!PyVectorcall_Function [<unknown file> @ <unknown line number>]\n00007FFEACD751B600007FFEACD75170 python312.dll!PyObject_Call [<unknown file> @ <unknown line number>]\n00007FFEACE99BC300007FFEACE92FF0 python312.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFEACD753F400007FFEACD753A0 python312.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n00007FFEACD74A7100007FFEACD749C0 python312.dll!PyObject_FastCallDictTstate [<unknown file> @ <unknown line number>]\n00007FFEACD756B500007FFEACD75610 python312.dll!PyObject_Call_Prepend [<unknown file> @ <unknown line number>]\n00007FFEACDF017200007FFEACDEBDF0 python312.dll!PyType_Ready [<unknown file> @ <unknown line number>]\n00007FFEACD7524600007FFEACD75170 python312.dll!PyObject_Call [<unknown file> @ <unknown line number>]\n00007FFEACE99BC300007FFEACE92FF0 python312.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFEACD753F400007FFEACD753A0 python312.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n00007FFEACD77AB100007FFEACD776A0 python312.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n00007FFEACD7810200007FFEACD78020 python312.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n00007FFEACD74F9900007FFEACD74EE0 python312.dll!PyVectorcall_Function [<unknown file> @ <unknown line number>]\n00007FFEACD751B600007FFEACD75170 python312.dll!PyObject_Call [<unknown file> @ <unknown line number>]\n00007FFEACE99BC300007FFEACE92FF0 python312.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFEACD753F400007FFEACD753A0 python312.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n00007FFEACD77AB100007FFEACD776A0 python312.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n00007FFEACD7810200007FFEACD78020 python312.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n00007FFEACD74F9900007FFEACD74EE0 python312.dll!PyVectorcall_Function [<unknown file> @ <unknown line number>]\n00007FFEACD751B600007FFEACD75170 python312.dll!PyObject_Call [<unknown file> @ <unknown line number>]\n00007FFEACE99BC300007FFEACE92FF0 python312.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFEACD753F400007FFEACD753A0 python312.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n00007FFEACD74A7100007FFEACD749C0 python312.dll!PyObject_FastCallDictTstate [<unknown file> @ <unknown line number>]\n00007FFEACD756B500007FFEACD75610 python312.dll!PyObject_Call_Prepend [<unknown file> @ <unknown line number>]\n00007FFEACDF017200007FFEACDEBDF0 python312.dll!PyType_Ready [<unknown file> @ <unknown line number>]\n00007FFEACD7524600007FFEACD75170 python312.dll!PyObject_Call [<unknown file> @ <unknown line number>]\n00007FFEACE99BC300007FFEACE92FF0 python312.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFEACD753F400007FFEACD753A0 python312.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n00007FFEACD77AB100007FFEACD776A0 python312.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n00007FFEACD781A100007FFEACD78020 python312.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n00007FFEACD7507600007FFEACD74EE0 python312.dll!PyVectorcall_Function [<unknown file> @ <unknown line number>]\n00007FFEACD751B600007FFEACD75170 python312.dll!PyObject_Call [<unknown file> @ <unknown line number>]\n00007FFEACE99BC300007FFEACE92FF0 python312.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFEACD753F400007FFEACD753A0 python312.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n00007FFEACD77AB100007FFEACD776A0 python312.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n00007FFEACD781A100007FFEACD78020 python312.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n00007FFEACD7507600007FFEACD74EE0 python312.dll!PyVectorcall_Function [<unknown file> @ <unknown line number>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(files):\n\u001b[0;32m      5\u001b[0m     image \u001b[38;5;241m=\u001b[39m load(file)\n\u001b[1;32m----> 6\u001b[0m     result, _ \u001b[38;5;241m=\u001b[39m \u001b[43mglip_demo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_on_web_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaption\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#imshow(result, caption)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     imsave(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRESULTS/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mfile\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\engine\\predictor_glip.py:140\u001b[0m, in \u001b[0;36mGLIPDemo.run_on_web_image\u001b[1;34m(self, original_image, original_caption, thresh, custom_entity, alpha)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_on_web_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[0;32m    135\u001b[0m         original_image, \n\u001b[0;32m    136\u001b[0m         original_caption, \n\u001b[0;32m    137\u001b[0m         thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m    138\u001b[0m         custom_entity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    139\u001b[0m         alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m):\n\u001b[1;32m--> 140\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_caption\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_entity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisualize_with_predictions(original_image, predictions, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, text_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.5\u001b[39m, text_pixel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\engine\\predictor_glip.py:234\u001b[0m, in \u001b[0;36mGLIPDemo.compute_prediction\u001b[1;34m(self, original_image, original_caption, custom_entity)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# compute predictions\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 234\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43moriginal_caption\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositive_map_label_to_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m [o\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_device) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m predictions]\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference time per image: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(timeit\u001b[38;5;241m.\u001b[39mtime\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m tic))\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\modeling\\detector\\generalized_vl_rcnn.py:284\u001b[0m, in \u001b[0;36mGeneralizedVLRCNN.forward\u001b[1;34m(self, images, targets, captions, positive_map, greenlight_map)\u001b[0m\n\u001b[0;32m    282\u001b[0m         proposal_losses \u001b[38;5;241m=\u001b[39m {(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrpn_null_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, null_loss)}\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     proposals, proposal_losses, fused_visual_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisual_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage_dict_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswint_feature_c4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads:\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mMODEL\u001b[38;5;241m.\u001b[39mROI_MASK_HEAD\u001b[38;5;241m.\u001b[39mPREDICTOR\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVL\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\modeling\\rpn\\vldyhead.py:920\u001b[0m, in \u001b[0;36mVLDyHeadModule.forward\u001b[1;34m(self, images, features, targets, language_dict_features, positive_map, captions, swint_feature_c4)\u001b[0m\n\u001b[0;32m    916\u001b[0m     language_dict_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedded\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m embedding\n\u001b[0;32m    917\u001b[0m     language_dict_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtunable_linear\u001b[38;5;241m.\u001b[39mweight[:embedding\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), :]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m language_dict_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    919\u001b[0m box_cls, box_regression, centerness, token_logits, \\\n\u001b[1;32m--> 920\u001b[0m proj_tokens, contrastive_logits, dot_product_logits, mlm_logits, shallow_img_emb_feats, fused_visual_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mlanguage_dict_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mswint_feature_c4\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchor_generator(images, features)\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\modeling\\rpn\\vldyhead.py:739\u001b[0m, in \u001b[0;36mVLDyHead.forward\u001b[1;34m(self, x, language_dict_features, embedding, swint_feature_c4)\u001b[0m\n\u001b[0;32m    734\u001b[0m centerness \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    736\u001b[0m feat_inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisual\u001b[39m\u001b[38;5;124m\"\u001b[39m: x,\n\u001b[0;32m    737\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m\"\u001b[39m: language_dict_features}\n\u001b[1;32m--> 739\u001b[0m dyhead_tower \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdyhead_tower\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;66;03m# soft token\u001b[39;00m\n\u001b[0;32m    742\u001b[0m t_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\modeling\\rpn\\vldyhead.py:205\u001b[0m, in \u001b[0;36mDyConv.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    202\u001b[0m     mask \u001b[38;5;241m=\u001b[39m offset_mask[:, \u001b[38;5;241m18\u001b[39m:, :, :]\u001b[38;5;241m.\u001b[39msigmoid()\n\u001b[0;32m    203\u001b[0m     conv_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(offset\u001b[38;5;241m=\u001b[39moffset, mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[1;32m--> 205\u001b[0m temp_fea \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDyConv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconv_args\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    208\u001b[0m     temp_fea\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDyConv[\u001b[38;5;241m2\u001b[39m](visual_feats[level \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconv_args))\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\modeling\\rpn\\vldyhead.py:135\u001b[0m, in \u001b[0;36mConv3x3Norm.forward\u001b[1;34m(self, input, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 135\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn:\n\u001b[0;32m    137\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(x)\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:466\u001b[0m, in \u001b[0;36mcustom_fwd.<locals>.decorate_fwd\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fwd(\n\u001b[0;32m    462\u001b[0m             \u001b[38;5;241m*\u001b[39m_cast(args, device_type, cast_inputs),\n\u001b[0;32m    463\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_cast(kwargs, device_type, cast_inputs),\n\u001b[0;32m    464\u001b[0m         )\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\layers\\deform_conv.py:380\u001b[0m, in \u001b[0;36mModulatedDeformConv.forward\u001b[1;34m(self, input, offset, mask)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;129m@custom_fwd\u001b[39m(cast_inputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, offset, mask):\n\u001b[1;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodulated_deform_conv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeformable_groups\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\autograd\\function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m     )\n",
      "File \u001b[1;32me:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\layers\\deform_conv.py:184\u001b[0m, in \u001b[0;36mModulatedDeformConvFunction.forward\u001b[1;34m(ctx, input, offset, mask, weight, bias, stride, padding, dilation, groups, deformable_groups)\u001b[0m\n\u001b[0;32m    181\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mnew_empty(\n\u001b[0;32m    182\u001b[0m     ModulatedDeformConvFunction\u001b[38;5;241m.\u001b[39m_infer_shape(ctx, \u001b[38;5;28minput\u001b[39m, weight))\n\u001b[0;32m    183\u001b[0m ctx\u001b[38;5;241m.\u001b[39m_bufs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mnew_empty(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mnew_empty(\u001b[38;5;241m0\u001b[39m)]\n\u001b[1;32m--> 184\u001b[0m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodulated_deform_conv_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bufs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bufs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeformable_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_bias\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Not compiled with GPU support\nException raised from modulated_deform_conv_forward at E:\\Git\\ML\\VideoAnalytics\\GLIP\\maskrcnn_benchmark\\csrc\\deform_conv.h:145 (most recent call first):\n00007FFEAA07462900007FFEAA074580 c10.dll!c10::Error::Error [<unknown file> @ <unknown line number>]\n00007FFEAA07416A00007FFEAA074110 c10.dll!c10::detail::torchCheckFail [<unknown file> @ <unknown line number>]\n00007FFDB3D7ABFA00007FFDB3D65C60 _C.cp312-win_amd64.pyd!c10::ivalue::Object::operator= [<unknown file> @ <unknown line number>]\n00007FFDB3D85C5100007FFDB3D7AE20 _C.cp312-win_amd64.pyd!PyInit__C [<unknown file> @ <unknown line number>]\n00007FFDB3D8238000007FFDB3D7AE20 _C.cp312-win_amd64.pyd!PyInit__C [<unknown file> @ <unknown line number>]\n00007FFDB3D8241400007FFDB3D7AE20 _C.cp312-win_amd64.pyd!PyInit__C [<unknown file> @ <unknown line number>]\n00007FFDB3D77E8D00007FFDB3D65C60 _C.cp312-win_amd64.pyd!c10::ivalue::Object::operator= [<unknown file> @ <unknown line number>]\n00007FFEACDC28E300007FFEACDC1A80 python312.dll!PyCFunction_GetFlags [<unknown file> @ <unknown line number>]\n00007FFEACD74DFE00007FFEACD74CC0 python312.dll!PyObject_MakeTpCall [<unknown file> @ <unknown line number>]\n00007FFEACD7512500007FFEACD750F0 python312.dll!PyObject_Vectorcall [<unknown file> @ <unknown line number>]\n00007FFEACE9985400007FFEACE92FF0 python312.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFEACD753F400007FFEACD753A0 python312.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n00007FFEACD7507600007FFEACD74EE0 python312.dll!PyVectorcall_Function [<unknown file> @ <unknown line number>]\n00007FFEACD751B600007FFEACD75170 python312.dll!PyObject_Call [<unknown file> @ <unknown line number>]\n00007FFDBAA368B100007FFDBAA23270 torch_python.dll!THPPointer<_frame>::THPPointer<_frame> [<unknown file> @ <unknown line number>]\n00007FFEACDC291E00007FFEACDC1A80 python312.dll!PyCFunction_GetFlags [<unknown file> @ <unknown line number>]\n00007FFEACD7524600007FFEACD75170 python312.dll!PyObject_Call [<unknown file> @ <unknown line number>]\n00007FFEACE99BC300007FFEACE92FF0 python312.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFEACD753F400007FFEACD753A0 python312.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n00007FFEACD77AB100007FFEACD776A0 python312.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n00007FFEACD7810200007FFEACD78020 python312.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n00007FFEACD74F9900007FFEACD74EE0 python312.dll!PyVectorcall_Function [<unknown file> @ <unknown line number>]\n00007FFEACD751B600007FFEACD75170 python312.dll!PyObject_Call [<unknown file> @ <unknown line number>]\n00007FFEACE99BC300007FFEACE92FF0 python312.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFEACD753F400007FFEACD753A0 python312.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n00007FFEACD77AB100007FFEACD776A0 python312.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n00007FFEACD7810200007FFEACD78020 python312.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n00007FFEACD74F9900007FFEACD74EE0 python312.dll!PyVectorcall_Function [<unknown file> @ <unknown line number>]\n00007FFEACD751B600007FFEACD75170 python312.dll!PyObject_Call [<unknown file> @ <unknown line number>]\n00007FFEACE99BC300007FFEACE92FF0 python312.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFEACD753F400007FFEACD753A0 python312.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n00007FFEACD74A7100007FFEACD749C0 python312.dll!PyObject_FastCallDictTstate [<unknown file> @ <unknown line number>]\n00007FFEACD756B500007FFEACD75610 python312.dll!PyObject_Call_Prepend [<unknown file> @ <unknown line number>]\n00007FFEACDF017200007FFEACDEBDF0 python312.dll!PyType_Ready [<unknown file> @ <unknown line number>]\n00007FFEACD7524600007FFEACD75170 python312.dll!PyObject_Call [<unknown file> @ <unknown line number>]\n00007FFEACE99BC300007FFEACE92FF0 python312.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFEACD753F400007FFEACD753A0 python312.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n00007FFEACD77AB100007FFEACD776A0 python312.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n00007FFEACD7810200007FFEACD78020 python312.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n00007FFEACD74F9900007FFEACD74EE0 python312.dll!PyVectorcall_Function [<unknown file> @ <unknown line number>]\n00007FFEACD751B600007FFEACD75170 python312.dll!PyObject_Call [<unknown file> @ <unknown line number>]\n00007FFEACE99BC300007FFEACE92FF0 python312.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFEACD753F400007FFEACD753A0 python312.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n00007FFEACD77AB100007FFEACD776A0 python312.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n00007FFEACD7810200007FFEACD78020 python312.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n00007FFEACD74F9900007FFEACD74EE0 python312.dll!PyVectorcall_Function [<unknown file> @ <unknown line number>]\n00007FFEACD751B600007FFEACD75170 python312.dll!PyObject_Call [<unknown file> @ <unknown line number>]\n00007FFEACE99BC300007FFEACE92FF0 python312.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFEACD753F400007FFEACD753A0 python312.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n00007FFEACD74A7100007FFEACD749C0 python312.dll!PyObject_FastCallDictTstate [<unknown file> @ <unknown line number>]\n00007FFEACD756B500007FFEACD75610 python312.dll!PyObject_Call_Prepend [<unknown file> @ <unknown line number>]\n00007FFEACDF017200007FFEACDEBDF0 python312.dll!PyType_Ready [<unknown file> @ <unknown line number>]\n00007FFEACD7524600007FFEACD75170 python312.dll!PyObject_Call [<unknown file> @ <unknown line number>]\n00007FFEACE99BC300007FFEACE92FF0 python312.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFEACD753F400007FFEACD753A0 python312.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n00007FFEACD77AB100007FFEACD776A0 python312.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n00007FFEACD781A100007FFEACD78020 python312.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n00007FFEACD7507600007FFEACD74EE0 python312.dll!PyVectorcall_Function [<unknown file> @ <unknown line number>]\n00007FFEACD751B600007FFEACD75170 python312.dll!PyObject_Call [<unknown file> @ <unknown line number>]\n00007FFEACE99BC300007FFEACE92FF0 python312.dll!PyEval_EvalFrameDefault [<unknown file> @ <unknown line number>]\n00007FFEACD753F400007FFEACD753A0 python312.dll!PyFunction_Vectorcall [<unknown file> @ <unknown line number>]\n00007FFEACD77AB100007FFEACD776A0 python312.dll!PyCell_Set [<unknown file> @ <unknown line number>]\n00007FFEACD781A100007FFEACD78020 python312.dll!PyMethod_Self [<unknown file> @ <unknown line number>]\n00007FFEACD7507600007FFEACD74EE0 python312.dll!PyVectorcall_Function [<unknown file> @ <unknown line number>]\n"
     ]
    }
   ],
   "source": [
    "#for files in [glob.glob(e) for e in ['INPUT/*.jpg', 'INPUT/*.png', 'INPUT/*.JPG', 'INPUT/*.PNG']]:\n",
    "# for files in [glob.glob(e) for e in ['INPUT/*.png']]:\n",
    "files = ['../data/input_image.png']\n",
    "for i, file in enumerate(files):\n",
    "    image = load(file)\n",
    "    result, _ = glip_demo.run_on_web_image(image, caption[i], 0.5)\n",
    "    #imshow(result, caption)\n",
    "    imsave(result, \"RESULTS/\"+file.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade171d9-3587-4400-9ab3-47ecac4b0627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249410c1-9052-4488-8ae7-b493c97e0ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
